{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.0702697962492488 ; Activation  <function leaky_relu at 0x000000A8C0CCC598>\n",
      "Learning rate per minibatch: 0.0702697962492488\n",
      "     2.66       2.66       0.56       0.56            25\n",
      "     1.89       1.51      0.587        0.6            75\n",
      "     1.26       0.79      0.554       0.53           175\n",
      "    0.911      0.603      0.499       0.45           375\n",
      "    0.755      0.609       0.44      0.385           775\n",
      "    0.639      0.527      0.342      0.247          1575\n",
      "    0.526      0.414       0.24       0.14          3175\n",
      "    0.418       0.31       0.17        0.1          6375\n",
      "    0.325      0.232      0.126     0.0816         12775\n",
      "    0.259      0.194     0.0992     0.0728         25575\n",
      " aggregate_loss: 6635.911038219929\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.4876705539784691 ; Activation  <function sigmoid at 0x000000A8C0CCC8C8>\n",
      "Learning rate per minibatch: 0.4876705539784691\n",
      "     1.02       1.02       0.64       0.64            25\n",
      "     1.15       1.21       0.52       0.46            75\n",
      "     1.09       1.05      0.509        0.5           175\n",
      "    0.909       0.75       0.48      0.455           375\n",
      "    0.655      0.416      0.319      0.168           775\n",
      "    0.469      0.288      0.214      0.113          1575\n",
      "    0.369      0.271      0.157      0.101          3175\n",
      "    0.314      0.259      0.128     0.0997          6375\n",
      "    0.284      0.254      0.112     0.0964         12775\n",
      "    0.263      0.242        0.1     0.0887         25575\n",
      " aggregate_loss: 6719.300133943558\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.4746109609646953 ; Activation  <function leaky_relu at 0x000000A8C0CCC598>\n",
      "Learning rate per minibatch: 0.4746109609646953\n",
      "    0.706      0.706       0.32       0.32            25\n",
      "     3.61       5.07      0.493       0.58            75\n",
      "     2.08      0.934      0.509       0.52           175\n",
      "      1.4      0.809      0.661      0.795           375\n",
      "     1.05      0.713      0.601      0.545           775\n",
      "    0.895      0.748      0.536      0.472          1575\n",
      "    0.774      0.654       0.48      0.425          3175\n",
      "    0.668      0.563      0.382      0.285          6375\n",
      "    0.499      0.332      0.262      0.143         12775\n",
      "    0.361      0.222      0.176     0.0903         25575\n",
      " aggregate_loss: 9225.084239840508\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.398800031323153 ; Activation  <function sigmoid at 0x000000A8C0CCC8C8>\n",
      "Learning rate per minibatch: 0.398800031323153\n",
      "     1.02       1.02        0.6        0.6            25\n",
      "     1.29       1.43       0.52       0.48            75\n",
      "     1.06       0.88      0.503       0.49           175\n",
      "    0.852      0.672      0.384       0.28           375\n",
      "    0.636      0.434      0.281      0.185           775\n",
      "    0.482      0.333      0.211      0.144          1575\n",
      "    0.385       0.29      0.157      0.103          3175\n",
      "     0.31      0.236       0.12     0.0841          6375\n",
      "    0.272      0.234      0.107     0.0931         12775\n",
      "    0.254      0.236     0.0967     0.0867         25575\n",
      " aggregate_loss: 6498.233988344669\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.03535207857670763 ; Activation  <function leaky_relu at 0x000000A8C0CCC598>\n",
      "Learning rate per minibatch: 0.03535207857670763\n",
      "     2.13       2.13       0.48       0.48            25\n",
      "     1.22      0.762      0.467       0.46            75\n",
      "     0.91      0.678      0.446       0.43           175\n",
      "    0.775      0.656       0.48       0.51           375\n",
      "    0.696      0.623      0.459       0.44           775\n",
      "    0.636      0.579      0.409       0.36          1575\n",
      "    0.582      0.528      0.328      0.247          3175\n",
      "    0.488      0.396      0.222      0.117          6375\n",
      "     0.38      0.271      0.148     0.0753         12775\n",
      "    0.297      0.214      0.112     0.0762         25575\n",
      " aggregate_loss: 7594.398328542709\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.33440739309422046 ; Activation  <function sigmoid at 0x000000A8C0CCC8C8>\n",
      "Learning rate per minibatch: 0.33440739309422046\n",
      "    0.676      0.676       0.48       0.48            25\n",
      "    0.643      0.627      0.373       0.32            75\n",
      "    0.764      0.854      0.411       0.44           175\n",
      "    0.767       0.77      0.349      0.295           375\n",
      "    0.608      0.459      0.279      0.212           775\n",
      "     0.45      0.297      0.192      0.109          1575\n",
      "    0.359      0.269      0.149      0.106          3175\n",
      "    0.295      0.232       0.12     0.0909          6375\n",
      "    0.274      0.254      0.107     0.0948         12775\n",
      "    0.253      0.232     0.0974     0.0874         25575\n",
      " aggregate_loss: 6474.467149734497\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.3574802329800919 ; Activation  <function sigmoid at 0x000000A8C0CCC8C8>\n",
      "Learning rate per minibatch: 0.3574802329800919\n",
      "    0.688      0.688       0.48       0.48            25\n",
      "    0.654      0.637      0.333       0.26            75\n",
      "    0.707      0.747      0.394       0.44           175\n",
      "    0.853       0.98      0.461       0.52           375\n",
      "    0.595      0.353      0.284      0.117           775\n",
      "    0.496      0.401      0.238      0.194          1575\n",
      "    0.389      0.283      0.171      0.105          3175\n",
      "     0.32      0.253      0.135     0.0991          6375\n",
      "    0.282      0.243      0.113     0.0917         12775\n",
      "    0.261       0.24        0.1     0.0873         25575\n",
      " aggregate_loss: 6673.6156794428825\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.42260814285682463 ; Activation  <function relu at 0x000000A8C0CCC378>\n",
      "Learning rate per minibatch: 0.42260814285682463\n",
      "     2.01       2.01       0.56       0.56            25\n",
      "     3.28       3.91      0.453        0.4            75\n",
      "     1.78      0.663      0.469       0.48           175\n",
      "     1.23      0.743      0.491       0.51           375\n",
      "    0.929      0.647      0.441      0.395           775\n",
      "    0.787       0.65      0.419      0.398          1575\n",
      "     0.73      0.674        0.4      0.382          3175\n",
      "    0.693      0.656      0.381      0.362          6375\n",
      "    0.651      0.609      0.353      0.325         12775\n",
      "    0.486      0.322      0.243      0.133         25575\n",
      " aggregate_loss: 12447.473598122597\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.38083205663728853 ; Activation  <function relu at 0x000000A8C0CCC378>\n",
      "Learning rate per minibatch: 0.38083205663728853\n",
      "    0.774      0.774       0.52       0.52            25\n",
      "     1.33        1.6      0.333       0.24            75\n",
      "    0.913      0.604      0.331       0.33           175\n",
      "    0.716      0.544      0.288       0.25           375\n",
      "     0.66      0.606      0.346        0.4           775\n",
      "    0.641      0.623       0.33      0.315          1575\n",
      "    0.601      0.562      0.296      0.262          3175\n",
      "    0.536      0.471      0.253       0.21          6375\n",
      "    0.416      0.296      0.186      0.119         12775\n",
      "    0.318       0.22      0.136      0.087         25575\n",
      " aggregate_loss: 8131.911628961563\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.4566423510349945 ; Activation  <function leaky_relu at 0x000000A8C0CCC598>\n",
      "Learning rate per minibatch: 0.4566423510349945\n",
      "        1          1       0.56       0.56            25\n",
      "     4.57       6.36      0.533       0.52            75\n",
      "     2.42      0.805      0.634       0.71           175\n",
      "     1.52      0.738      0.683      0.725           375\n",
      "     1.15      0.802      0.588        0.5           775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.937       0.73      0.536      0.485          1575\n",
      "    0.796      0.657      0.477      0.419          3175\n",
      "    0.688      0.581      0.398       0.32          6375\n",
      "    0.527      0.367      0.281      0.165         12775\n",
      "    0.372      0.218      0.185     0.0881         25575\n",
      " aggregate_loss: 9524.383234262466\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.32335238260381854 ; Activation  <function relu at 0x000000A8C0CCC378>\n",
      "Learning rate per minibatch: 0.32335238260381854\n",
      "    0.861      0.861       0.52       0.52            25\n",
      "     3.67       5.08      0.587       0.62            75\n",
      "     1.96      0.667       0.52       0.47           175\n",
      "     1.24      0.621      0.437      0.365           375\n",
      "    0.916      0.608      0.363      0.292           775\n",
      "    0.722      0.534      0.318      0.275          1575\n",
      "    0.628      0.535      0.284       0.25          3175\n",
      "    0.509      0.392      0.231      0.179          6375\n",
      "    0.392      0.275       0.17      0.109         12775\n",
      "    0.308      0.224      0.131     0.0917         25575\n",
      " aggregate_loss: 7893.127264082432\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.4808379028216496 ; Activation  <function leaky_relu at 0x000000A8C0CCC598>\n",
      "Learning rate per minibatch: 0.4808379028216496\n",
      "    0.855      0.855       0.56       0.56            25\n",
      "     1.86       2.36      0.453        0.4            75\n",
      "     1.17      0.652      0.474       0.49           175\n",
      "    0.904      0.672      0.459      0.445           375\n",
      "    0.804       0.71      0.427      0.398           775\n",
      "    0.762      0.721      0.409      0.391          1575\n",
      "    0.703      0.644      0.375      0.341          3175\n",
      "    0.637      0.572      0.325      0.275          6375\n",
      "    0.516      0.396      0.244      0.163         12775\n",
      "    0.368       0.22      0.164     0.0842         25575\n",
      " aggregate_loss: 9415.288614749908\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.11879564048752202 ; Activation  <function relu at 0x000000A8C0CCC378>\n",
      "Learning rate per minibatch: 0.11879564048752202\n",
      "    0.649      0.649       0.48       0.48            25\n",
      "      1.2       1.48      0.493        0.5            75\n",
      "     1.38       1.51      0.503       0.51           175\n",
      "    0.967      0.606      0.411       0.33           375\n",
      "    0.766      0.578      0.321      0.237           775\n",
      "    0.605      0.449      0.248      0.176          1575\n",
      "    0.473      0.343      0.175      0.104          3175\n",
      "     0.37      0.269      0.135     0.0953          6375\n",
      "    0.301      0.232      0.113     0.0905         12775\n",
      "     0.25      0.199     0.0942     0.0756         25575\n",
      " aggregate_loss: 6397.856254816055\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.200412203026577 ; Activation  <function leaky_relu at 0x000000A8C0CCC598>\n",
      "Learning rate per minibatch: 0.200412203026577\n",
      "     1.27       1.27       0.64       0.64            25\n",
      "     2.15       2.58      0.493       0.42            75\n",
      "     1.26      0.595      0.429       0.38           175\n",
      "    0.894      0.573      0.419       0.41           375\n",
      "    0.694      0.508      0.312      0.212           775\n",
      "    0.564      0.438      0.245       0.18          1575\n",
      "    0.451      0.339      0.186      0.129          3175\n",
      "    0.363      0.276       0.15      0.114          6375\n",
      "    0.295      0.228       0.12     0.0891         12775\n",
      "    0.254      0.212        0.1     0.0808         25575\n",
      " aggregate_loss: 6490.144449710846\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.09286614692920138 ; Activation  <function leaky_relu at 0x000000A8C0CCC598>\n",
      "Learning rate per minibatch: 0.09286614692920138\n",
      "    0.679      0.679        0.4        0.4            25\n",
      "    0.651      0.637      0.427       0.44            75\n",
      "     0.88       1.05      0.446       0.46           175\n",
      "    0.747       0.63      0.445      0.445           375\n",
      "    0.642      0.544      0.386       0.33           775\n",
      "    0.568      0.495      0.288      0.194          1575\n",
      "    0.482      0.397      0.211      0.135          3175\n",
      "    0.383      0.284      0.157      0.103          6375\n",
      "    0.305      0.228       0.12     0.0823         12775\n",
      "    0.249      0.193     0.0958      0.072         25575\n",
      " aggregate_loss: 6380.979130625725\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.4601742058849012 ; Activation  <function relu at 0x000000A8C0CCC378>\n",
      "Learning rate per minibatch: 0.4601742058849012\n",
      "     1.79       1.79       0.64       0.64            25\n",
      "     4.25       5.47      0.533       0.48            75\n",
      "     2.22      0.693      0.514        0.5           175\n",
      "     1.39      0.661      0.472      0.435           375\n",
      "    0.986       0.61      0.421      0.372           775\n",
      "     0.84      0.699       0.42      0.419          1575\n",
      "    0.762      0.685      0.413      0.406          3175\n",
      "    0.722      0.682      0.437      0.461          6375\n",
      "    0.711      0.701      0.466      0.495         12775\n",
      "    0.701       0.69      0.475      0.484         25575\n",
      " aggregate_loss: 17934.923029899597\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.13142109580050446 ; Activation  <function relu at 0x000000A8C0CCC378>\n",
      "Learning rate per minibatch: 0.13142109580050446\n",
      "     1.11       1.11       0.48       0.48            25\n",
      "    0.921      0.824      0.453       0.44            75\n",
      "    0.786      0.685      0.446       0.44           175\n",
      "    0.677      0.582        0.4       0.36           375\n",
      "    0.617      0.561      0.334      0.273           775\n",
      "    0.556      0.498      0.263      0.195          1575\n",
      "    0.457      0.358       0.19      0.117          3175\n",
      "    0.359      0.262      0.141     0.0925          6375\n",
      "     0.29      0.222      0.111     0.0817         12775\n",
      "    0.248      0.206     0.0966      0.082         25575\n",
      " aggregate_loss: 6343.74319756031\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.3042986702364216 ; Activation  <function sigmoid at 0x000000A8C0CCC8C8>\n",
      "Learning rate per minibatch: 0.3042986702364216\n",
      "     0.74       0.74       0.56       0.56            25\n",
      "     1.18       1.41      0.587        0.6            75\n",
      "     1.33       1.45      0.571       0.56           175\n",
      "    0.892      0.504      0.395       0.24           375\n",
      "    0.642      0.409      0.272      0.158           775\n",
      "    0.478       0.32      0.189      0.109          1575\n",
      "    0.373      0.269      0.139     0.0894          3175\n",
      "    0.308      0.244      0.117     0.0959          6375\n",
      "    0.271      0.235      0.101     0.0852         12775\n",
      "    0.246       0.22     0.0895     0.0778         25575\n",
      " aggregate_loss: 6290.111019253731\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.09408216236650714 ; Activation  <function relu at 0x000000A8C0CCC378>\n",
      "Learning rate per minibatch: 0.09408216236650714\n",
      "     1.55       1.55       0.48       0.48            25\n",
      "     1.17      0.973      0.547       0.58            75\n",
      "     1.19       1.21      0.549       0.55           175\n",
      "    0.872      0.595      0.499      0.455           375\n",
      "    0.707      0.552      0.412       0.33           775\n",
      "    0.601      0.498      0.319       0.23          1575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.492      0.385      0.216      0.115          3175\n",
      "    0.397      0.303      0.163       0.11          6375\n",
      "     0.31      0.223      0.122     0.0819         12775\n",
      "    0.254      0.198     0.0986      0.075         25575\n",
      " aggregate_loss: 6498.674899876118\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate  0.23399684458170433 ; Activation  <function leaky_relu at 0x000000A8C0CCC598>\n",
      "Learning rate per minibatch: 0.23399684458170433\n",
      "    0.976      0.976       0.24       0.24            25\n",
      "      1.4       1.62      0.427       0.52            75\n",
      "     1.18       1.02      0.394       0.37           175\n",
      "    0.861      0.579       0.36       0.33           375\n",
      "    0.666      0.484      0.288       0.22           775\n",
      "    0.533      0.405      0.217      0.149          1575\n",
      "    0.443      0.355      0.185      0.154          3175\n",
      "    0.384      0.325      0.162      0.139          6375\n",
      "    0.306      0.228      0.125     0.0889         12775\n",
      "    0.258       0.21      0.104     0.0822         25575\n",
      " aggregate_loss: 6604.738355994225\n",
      "{'lr': 0.3042986702364216, 'act': 0}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import cntk as C\n",
    "from cntk.learners import sgd, learning_rate_schedule, UnitType\n",
    "from cntk.logging import ProgressPrinter\n",
    "from cntk.layers import Dense, Sequential\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "def generate_random_data(sample_size, feature_dim, num_classes):\n",
    "     # Create synthetic data using NumPy.\n",
    "     Y = np.random.randint(size=(sample_size, 1), low=0, high=num_classes)\n",
    "\n",
    "     # Make sure that the data is separable\n",
    "     X = (np.random.randn(sample_size, feature_dim) + 3) * (Y + 1)\n",
    "     X = X.astype(np.float32)\n",
    "     # converting class 0 into the vector \"1 0 0\",\n",
    "     # class 1 into vector \"0 1 0\", ...\n",
    "     class_ind = [Y == class_number for class_number in range(num_classes)]\n",
    "     Y = np.asarray(np.hstack(class_ind), dtype=np.float32)\n",
    "     return X, Y\n",
    "\n",
    "def ffnet(args):\n",
    "    lr, act = args\n",
    "    inputs = 2\n",
    "    outputs = 2\n",
    "    layers = 2\n",
    "    hidden_dimension = 50\n",
    "    if act == 0:\n",
    "        activ = C.sigmoid\n",
    "    elif act == 1:\n",
    "        activ = C.relu\n",
    "    elif act == 2:\n",
    "        activ = C.leaky_relu\n",
    "        \n",
    "    # input variables denoting the features and label data\n",
    "    features = C.input((inputs), np.float32)\n",
    "    label = C.input((outputs), np.float32)\n",
    "\n",
    "    # Instantiate the feedforward classification model\n",
    "    \n",
    "    my_model = Sequential ([\n",
    "                    Dense(hidden_dimension, activation=activ),\n",
    "                    Dense(outputs)])\n",
    "    z = my_model(features)\n",
    "\n",
    "    ce = C.cross_entropy_with_softmax(z, label)\n",
    "    pe = C.classification_error(z, label)\n",
    "\n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    lr_per_minibatch = learning_rate_schedule(lr, UnitType.minibatch)\n",
    "    progress_printer = ProgressPrinter(0)\n",
    "    trainer = C.Trainer(z, (ce, pe), [sgd(z.parameters, lr=lr_per_minibatch)], [progress_printer])\n",
    "\n",
    "    # Get minibatches of training data and perform model training\n",
    "    minibatch_size = 25\n",
    "    num_minibatches_to_train = 1024\n",
    "    aggregate_loss = 0.0\n",
    "    print (\"Learning rate \", lr, \"; Activation \", activ)\n",
    "    for i in range(num_minibatches_to_train):\n",
    "        train_features, labels = generate_random_data(minibatch_size, inputs, outputs)\n",
    "        # Specify the mapping of input variables in the model to actual minibatch data to be trained with\n",
    "        trainer.train_minibatch({features : train_features, label : labels})\n",
    "        sample_count = trainer.previous_minibatch_sample_count\n",
    "        aggregate_loss += trainer.previous_minibatch_loss_average * sample_count\n",
    "\n",
    "    last_avg_error = aggregate_loss / trainer.total_number_of_samples_seen\n",
    "\n",
    "    test_features, test_labels = generate_random_data(minibatch_size, inputs, outputs)\n",
    "    avg_error = trainer.test_minibatch({features : test_features, label : test_labels})\n",
    "    print(' aggregate_loss: {}'.format(aggregate_loss))\n",
    "    return aggregate_loss\n",
    "\n",
    "np.random.seed(98052)\n",
    "trials = Trials()\n",
    "best = fmin(ffnet,\n",
    "    space=[hp.uniform('lr', 0.01, 0.5), hp.randint('act', 3)],\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=20,\n",
    "    trials=trials)\n",
    "\n",
    "print (best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
